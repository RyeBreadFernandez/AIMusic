{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AnimeLSTM\n",
    "Author: [@neelr](https://github.com/neelr)  \n",
    "Dataset: https://www.kaggle.com/programgeek01/anime-music-midi/  \n",
    "\n",
    "In summary what this notebook tries to do is generate anime music from MIDI files using an LSTM neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import numpy as np\n",
    "import re\n",
    "import glob\n",
    "from time import time\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import TensorBoard\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Dropout\n",
    "from tensorflow.keras.layers import LSTM\n",
    "from tensorflow.keras.layers import Activation\n",
    "from tensorflow.keras.layers import BatchNormalization as BatchNorm\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras_tqdm import TQDMNotebookCallback\n",
    "from music21 import converter, instrument, note, chord, stream, tempo\n",
    "import music21"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check all devices attached, make sure there is a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devices(\n",
    "    device_type=None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse each midi file and store all notes/chords/rests/metronome/time signature/key in the `notes` array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "notes = []\n",
    "allFiles = glob.glob(\"data/*.mid\")\n",
    "for file in tqdm(allFiles):\n",
    "    midi = converter.parse(file)\n",
    "    notes_to_parse = None\n",
    "    parts = instrument.partitionByInstrument(midi)\n",
    "    if parts: # file has instrument parts\n",
    "        notes_to_parse = parts.parts[0].recurse()\n",
    "    else: # file has notes in a flat structure\n",
    "        notes_to_parse = midi.flat.notes\n",
    "    for sound in notes_to_parse:\n",
    "        if isinstance(sound, note.Note):\n",
    "            notes.append(str(sound.pitch))\n",
    "        elif isinstance(sound, chord.Chord):\n",
    "            notes.append('.'.join(str(n) for n in sound.normalOrder))\n",
    "        elif isinstance(sound, note.Rest):\n",
    "            notes.append(\"rest\")\n",
    "        elif isinstance(sound, tempo.MetronomeMark):\n",
    "            notes.append(f\"metronome-{sound.number}-{sound.text}\")\n",
    "        elif isinstance(sound, music21.meter.TimeSignature):\n",
    "            notes.append(f\"timesig-{sound.ratioString}\")\n",
    "        elif isinstance(sound, music21.key.Key):\n",
    "            notes.append(f\"key-{str(sound)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the notes in notes.npy and turn into numpy array for easy use later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"numpy_array_saves/notes.npy\", notes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parse the data (`notes` array) into LSTM compatible data (uses past 200 notes to predict next 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 200\n",
    "\n",
    "# get all pitch names\n",
    "pitchnames = sorted(set(item for item in notes))\n",
    "# create a dictionary to map pitches to integers\n",
    "note_to_int = dict((note, number) for number, note in enumerate(pitchnames))\n",
    "network_input = []\n",
    "network_output = []\n",
    "# create input sequences and the corresponding outputs\n",
    "for i in range(0, len(notes) - sequence_length, 1):\n",
    "    sequence_in = notes[i:i + sequence_length]\n",
    "    sequence_out = notes[i + sequence_length]\n",
    "    network_input.append([note_to_int[char] for char in sequence_in])\n",
    "    network_output.append(note_to_int[sequence_out])\n",
    "n_patterns = len(network_input)\n",
    "# reshape the input into a format compatible with LSTM layers\n",
    "network_input = np.reshape(network_input, (n_patterns, sequence_length, 1))\n",
    "# normalize input\n",
    "non_normalized_input = network_input\n",
    "network_input = network_input / float(len(pitchnames))\n",
    "network_output = to_categorical(network_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pitchnames) # number of outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decide model structure\n",
    "Main differences between v1, v2, and v3 are that v1 had 100 note lookback, and 88 samples, while v2 had a 200 note lookback and ~130 samples. The big jump from v2 to v3 is that I added melodies, time signatures, and keys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(\n",
    "    300,\n",
    "    input_shape=(network_input.shape[1], network_input.shape[2]),\n",
    "    return_sequences=True\n",
    "))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(512, return_sequences=True))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(LSTM(700))\n",
    "model.add(Dense(850))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(Dense(len(pitchnames)))\n",
    "model.add(Activation('softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer=\"adam\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = \"v3-{epoch:02d}-{loss:.4f}-bigger.hdf5\"    \n",
    "checkpoint = ModelCheckpoint(\n",
    "    filepath, monitor='loss', \n",
    "    verbose=0,        \n",
    "    save_best_only=True,    \n",
    "    mode='min'\n",
    ")\n",
    "tensorboard = TensorBoard(log_dir=\"logs\\{}\".format(time()), profile_batch=0, update_freq='epoch')\n",
    "# Take out checkpoint if you don't want to save the best weights, and tensorboard if you don't have that running\n",
    "callbacks_list = [checkpoint, TQDMNotebookCallback(), tensorboard]\n",
    "\n",
    "# Verbose 0 because keras TQDM is a better UI + keras' loading bars don't work for me locally\n",
    "model.fit(network_input, network_output, epochs=200, batch_size=64, callbacks=callbacks_list,verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip the whole training and load weights if you want\n",
    "model.load(\"models/Anime-LSTM-Model-v3.hdf5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generation\n",
    "Here we use the model to generate 1000 notes. Bellow we predict each note and use the `int_to_note` dictionary to map the outputs to the corresponding string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = np.random.randint(0, len(network_input)-1)\n",
    "int_to_note = dict((number, note) for number, note in enumerate(pitchnames))\n",
    "pattern = non_normalized_input[start]\n",
    "prediction_output = []\n",
    "# generate 1000 notes\n",
    "for note_index in tqdm(range(1000)):\n",
    "    prediction_input = np.reshape(pattern,(1,pattern.shape[0],1))\n",
    "    prediction_input = prediction_input / float(len(pitchnames))\n",
    "    prediction = model.predict(prediction_input, verbose=0)\n",
    "    index = np.argmax(prediction)\n",
    "    result = int_to_note[index]\n",
    "    prediction_output.append(result)\n",
    "    pattern = np.append(pattern,[index])\n",
    "    pattern = pattern[1:len(pattern)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we take the output strings and parse them into the corresponding music21 classes, and add then to the `output_notes` array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "offset = 0\n",
    "output_notes = []\n",
    "# create note and chord objects based on the values generated by the model\n",
    "for pattern in prediction_output:\n",
    "    # Pattern is a chord\n",
    "    if (('.' in pattern) or pattern.isdigit()) and not ('metronome' in pattern):\n",
    "        notes_in_chord = pattern.split('.')\n",
    "        notes = []\n",
    "        for current_note in notes_in_chord:\n",
    "            new_note = note.Note(int(current_note))\n",
    "            new_note.storedInstrument = instrument.Piano()\n",
    "            notes.append(new_note)\n",
    "        new_chord = chord.Chord(notes)\n",
    "        new_chord.offset = offset\n",
    "        output_notes.append(new_chord)\n",
    "    # Pattern is a rest\n",
    "    elif pattern == \"rest\":\n",
    "        new_note = note.Rest()\n",
    "        new_note.offset = offset\n",
    "        new_note.storedInstrument = instrument.Piano()\n",
    "        output_notes.append(new_note)\n",
    "    # Pattern is a key\n",
    "    elif ('key' in pattern):\n",
    "        key = music21.key.Key(re.findall(r\"-(.*) \",pattern)[0], re.findall(r\" (.*)\",pattern)[0])\n",
    "        output_notes.append(key)\n",
    "    # Pattern is a metronome\n",
    "    elif ('metronome' in pattern):\n",
    "        metronome = music21.tempo.MetronomeMark(re.findall(r\"-.*?(-(.*))\",pattern)[0][1], float(re.findall(r\"-(.*)-\",pattern)[0]))\n",
    "        output_notes.append(metronome)\n",
    "    # Pattern is timesig\n",
    "    elif ('timesig' in pattern):\n",
    "        timesig = music21.meter.TimeSignature(re.findall(r\"-(.*)\",pattern))\n",
    "        output_notes.append(timesig)\n",
    "    # If none, then pattern is a note\n",
    "    else:\n",
    "        new_note = note.Note(pattern)\n",
    "        new_note.offset = offset\n",
    "        new_note.storedInstrument = instrument.Piano()\n",
    "        output_notes.append(new_note)\n",
    "    # increase offset each iteration so that notes do not stack\n",
    "    # Can change to edit the speed, but 0.5 is the norm\n",
    "    offset += 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly we take the output notes and create a MIDI stream and save it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "midi_stream = stream.Stream(output_notes)\n",
    "midi_stream.write('midi', fp=f'anime_music_{time()}.mid')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
